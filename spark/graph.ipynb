{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# グラフ\n",
    "\n",
    "ここの内容だけしかありません→ https://www.creationline.com/lab/10827\n",
    "(途中からエラーになる)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "playerArray = Array((1,(Messi,FW)), (2,(Suarez,FW)), (3,(Neymar,FW)), (4,(Iniesta,MF)), (5,(Rakitic,MF)), (6,(Busquets,MF)), (7,(Alves,DF)), (8,(Pique,DF)), (9,(Mascherano,DF)), (10,(Alba,DF)), (11,(Stegen,GK)))\n",
       "passArray = Array(Edge(1,2,8), Edge(1,3,12), Edge(1,4,7), Edge(1,5,6), Edge(1,6,10), Edge(1,7,10), Edge(1,8,1), Edge(1,10,7), Edge(2,1,6), Edge(2,3,6), Edge(2,4,2), Edge(2,5,3), Edge(2,6,1), Edge(2,7,3), Edge(2,8,1), Edge(3,1,10), Edge(3,2,6), Edge(3,4,6), Edge(3,5,1), Edge(3,6,3), Edge(3,7,3), Edge(3,10,9), Edge(4,1,6), Edge(4,2,2), Edge(4,3,10), Edge(4,5,3), Edge(4,6,7), Edge(4,7,2), Edge(4,8,1), Edge(4,9,4), Edge(4,10,14), Edge(5,1,12), Edge(5,2,2), Edge(5,3,1), Edge(5,4,2), Edge(5,6,5), Edge(5,7,14), ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(Edge(1,2,8), Edge(1,3,12), Edge(1,4,7), Edge(1,5,6), Edge(1,6,10), Edge(1,7,10), Edge(1,8,1), Edge(1,10,7), Edge(2,1,6), Edge(2,3,6), Edge(2,4,2), Edge(2,5,3), Edge(2,6,1), Edge(2,7,3), Edge(2,8,1), Edge(3,1,10), Edge(3,2,6), Edge(3,4,6), Edge(3,5,1), Edge(3,6,3), Edge(3,7,3), Edge(3,10,9), Edge(4,1,6), Edge(4,2,2), Edge(4,3,10), Edge(4,5,3), Edge(4,6,7), Edge(4,7,2), Edge(4,8,1), Edge(4,9,4), Edge(4,10,14), Edge(5,1,12), Edge(5,2,2), Edge(5,3,1), Edge(5,4,2), Edge(5,6,5), Edge(5,7,14), ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val playerArray = Array( (1L, (\"Messi\", \"FW\")), (2L, (\"Suarez\", \"FW\")), (3L, (\"Neymar\", \"FW\")), (4L, (\"Iniesta\", \"MF\")), (5L, (\"Rakitic\", \"MF\")), (6L, (\"Busquets\", \"MF\")), (7L, (\"Alves\", \"DF\")), (8L, (\"Pique\", \"DF\")), (9L, (\"Mascherano\", \"DF\")), (10L, (\"Alba\", \"DF\")), (11L, (\"Stegen\", \"GK\")) )\n",
    "val passArray = Array( Edge(1L, 2L, 8), Edge(1L, 3L, 12), Edge(1L, 4L, 7), Edge(1L, 5L, 6), Edge(1L, 6L, 10), Edge(1L, 7L, 10), Edge(1L, 8L, 1), Edge(1L, 10L, 7), Edge(2L, 1L, 6), Edge(2L, 3L, 6), Edge(2L, 4L, 2), Edge(2L, 5L, 3), Edge(2L, 6L, 1), Edge(2L, 7L, 3), Edge(2L, 8L, 1), Edge(3L, 1L, 10), Edge(3L, 2L, 6), Edge(3L, 4L, 6), Edge(3L, 5L, 1), Edge(3L, 6L, 3), Edge(3L, 7L, 3), Edge(3L, 10L, 9), Edge(4L, 1L, 6), Edge(4L, 2L, 2), Edge(4L, 3L, 10), Edge(4L, 5L, 3), Edge(4L, 6L, 7), Edge(4L, 7L, 2), Edge(4L, 8L, 1), Edge(4L, 9L, 4), Edge(4L, 10L, 14), Edge(5L, 1L, 12), Edge(5L, 2L, 2), Edge(5L, 3L, 1), Edge(5L, 4L, 2), Edge(5L, 6L, 5), Edge(5L, 7L, 14), Edge(5L, 8L, 3), Edge(5L, 9L, 5), Edge(5L, 10L, 1), Edge(6L, 1L, 8), Edge(6L, 2L, 2), Edge(6L, 4L, 9), Edge(6L, 5L, 8), Edge(6L, 7L, 16), Edge(6L, 8L, 5), Edge(6L, 9L, 7), Edge(6L, 10L, 5), Edge(6L, 11L, 2), Edge(7L, 1L, 25), Edge(7L, 2L, 2), Edge(7L, 4L, 1), Edge(7L, 5L, 21), Edge(7L, 6L, 7), Edge(7L, 8L, 3), Edge(7L, 9L, 2), Edge(7L, 10L, 4), Edge(8L, 1L, 2), Edge(8L, 2L, 2), Edge(8L, 4L, 1), Edge(8L, 5L, 1), Edge(8L, 6L, 1), Edge(8L, 7L, 12), Edge(8L, 9L, 6), Edge(8L, 10L, 3), Edge(8L, 11L, 6), Edge(9L, 1L, 1), Edge(9L, 2L, 1), Edge(9L, 3L, 1), Edge(9L, 4L, 4), Edge(9L, 5L, 3), Edge(9L, 6L, 7), Edge(9L, 7L, 7), Edge(9L, 8L, 4), Edge(9L, 10L, 10), Edge(9L, 11L, 2), Edge(10L, 2L, 2), Edge(10L, 3L, 21), Edge(10L, 4L, 14), Edge(10L, 6L, 8), Edge(10L, 7L, 1), Edge(10L, 9L, 13), Edge(10L, 11L, 1), Edge(11L, 1L, 1), Edge(11L, 5L, 1), Edge(11L, 6L, 1), Edge(11L, 7L, 2), Edge(11L, 8L, 1), Edge(11L, 9L, 1), Edge(11L, 10L, 4) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "playerRDD = ParallelCollectionRDD[0] at parallelize at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at <console>:36"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val playerRDD: RDD[(Long, (String, String))] = sc.parallelize(playerArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "passRDD = ParallelCollectionRDD[1] at parallelize at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1] at parallelize at <console>:36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val passRDD: RDD[Edge[Int]] = sc.parallelize(passArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph = org.apache.spark.graphx.impl.GraphImpl@7a324160\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@7a324160"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val graph: Graph[(String, String), Int] = Graph(playerRDD, passRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rakitic passed to Alves\n",
      "Rakitic passed to Pique\n",
      "Rakitic passed to Mascherano\n",
      "Rakitic passed to Alba\n",
      "Busquets passed to Messi\n",
      "Busquets passed to Suarez\n",
      "Busquets passed to Iniesta\n",
      "Busquets passed to Rakitic\n",
      "Busquets passed to Alves\n",
      "Busquets passed to Pique\n",
      "Busquets passed to Mascherano\n",
      "Busquets passed to Alba\n",
      "Busquets passed to Stegen\n",
      "Alves passed to Messi\n",
      "Alves passed to Suarez\n",
      "Alves passed to Iniesta\n",
      "Alves passed to Rakitic\n",
      "Alves passed to Busquets\n",
      "Alves passed to Pique\n",
      "Alves passed to Mascherano\n",
      "Alves passed to Alba\n",
      "Pique passed to Messi\n",
      "Pique passed to Suarez\n",
      "Pique passed to Iniesta\n",
      "Pique passed to Rakitic\n",
      "Pique passed to Busquets\n",
      "Pique passed to Alves\n",
      "Pique passed to Mascherano\n",
      "Pique passed to Alba\n",
      "Pique passed to Stegen\n",
      "Mascherano passed to Messi\n",
      "Mascherano passed to Suarez\n",
      "Mascherano passed to Neymar\n",
      "Mascherano passed to Iniesta\n",
      "Mascherano passed to Rakitic\n",
      "Mascherano passed to Busquets\n",
      "Mascherano passed to Alves\n",
      "Mascherano passed to Pique\n",
      "Mascherano passed to Alba\n",
      "Mascherano passed to Stegen\n",
      "Alba passed to Suarez\n",
      "Alba passed to Neymar\n",
      "Alba passed to Iniesta\n",
      "Alba passed to Busquets\n",
      "Alba passed to Alves\n",
      "Alba passed to Mascherano\n",
      "Alba passed to Stegen\n",
      "Stegen passed to Messi\n",
      "Stegen passed to Rakitic\n",
      "Stegen passed to Busquets\n",
      "Stegen passed to Alves\n",
      "Stegen passed to Pique\n",
      "Stegen passed to Mascherano\n",
      "Stegen passed to Alba\n",
      "Messi passed to Suarez\n",
      "Messi passed to Neymar\n",
      "Messi passed to Iniesta\n",
      "Messi passed to Rakitic\n",
      "Messi passed to Busquets\n",
      "Messi passed to Alves\n",
      "Messi passed to Pique\n",
      "Messi passed to Alba\n",
      "Suarez passed to Messi\n",
      "Suarez passed to Neymar\n",
      "Suarez passed to Iniesta\n",
      "Suarez passed to Rakitic\n",
      "Suarez passed to Busquets\n",
      "Suarez passed to Alves\n",
      "Suarez passed to Pique\n",
      "Neymar passed to Messi\n",
      "Neymar passed to Suarez\n",
      "Neymar passed to Iniesta\n",
      "Neymar passed to Rakitic\n",
      "Neymar passed to Busquets\n",
      "Neymar passed to Alves\n",
      "Neymar passed to Alba\n",
      "Iniesta passed to Messi\n",
      "Iniesta passed to Suarez\n",
      "Iniesta passed to Neymar\n",
      "Iniesta passed to Rakitic\n",
      "Iniesta passed to Busquets\n",
      "Iniesta passed to Alves\n",
      "Iniesta passed to Pique\n",
      "Iniesta passed to Mascherano\n",
      "Iniesta passed to Alba\n",
      "Rakitic passed to Messi\n",
      "Rakitic passed to Suarez\n",
      "Rakitic passed to Neymar\n",
      "Rakitic passed to Iniesta\n",
      "Rakitic passed to Busquets\n"
     ]
    }
   ],
   "source": [
    "for (triplet <- graph.triplets.collect) { println(s\"${triplet.srcAttr._1} passed to ${triplet.dstAttr._1}\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messi passed to Neymar 10 times or more\n",
      "Messi passed to Busquets 10 times or more\n",
      "Messi passed to Alves 10 times or more\n",
      "Neymar passed to Messi 10 times or more\n",
      "Iniesta passed to Neymar 10 times or more\n",
      "Iniesta passed to Alba 10 times or more\n",
      "Rakitic passed to Messi 10 times or more\n",
      "Rakitic passed to Alves 10 times or more\n",
      "Busquets passed to Alves 10 times or more\n",
      "Alves passed to Messi 10 times or more\n",
      "Alves passed to Rakitic 10 times or more\n",
      "Pique passed to Alves 10 times or more\n",
      "Mascherano passed to Alba 10 times or more\n",
      "Alba passed to Neymar 10 times or more\n",
      "Alba passed to Iniesta 10 times or more\n",
      "Alba passed to Mascherano 10 times or more\n"
     ]
    }
   ],
   "source": [
    "for (triplet <- graph.triplets.filter(t => t.attr >= 10).collect) {\n",
    "    println(s\"${triplet.srcAttr._1} passed to ${triplet.dstAttr._1} 10 times or more\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Player\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Player(name: String, position: String, inDeg: Int, outDeg: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iniPlayerGraph = org.apache.spark.graphx.impl.GraphImpl@93a640f\n",
       "PlayerGraph = org.apache.spark.graphx.impl.GraphImpl@5610ff4a\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@5610ff4a"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iniPlayerGraph: Graph[Player, Int] = graph.mapVertices{ case (id, (name, position)) => Player(name, position, 0, 0) }\n",
    "val PlayerGraph = iniPlayerGraph.outerJoinVertices(iniPlayerGraph.inDegrees) { case (id, u, inDegOpt) => Player(u.name, u.position, inDegOpt.getOrElse(0), u.outDeg) }.outerJoinVertices(iniPlayerGraph.outDegrees) { case (id, u, outDegOpt) => Player(u.name, u.position, u.inDeg, outDegOpt.getOrElse(0)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: ClassNotFound with classloader: scala.reflect.internal.util.ScalaClassLoader$URLClassLoader@6c00de9f\n",
       "StackTrace:   at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ((id, property) <- PlayerGraph.vertices.collect) {\n",
    "    println(s\"${property.name} recieved a pass from ${property.inDeg} people\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: ClassNotFound with classloader: scala.reflect.internal.util.ScalaClassLoader$URLClassLoader@6c00de9f\n",
       "StackTrace:   at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ((id, property) <- PlayerGraph.vertices.collect) {\n",
    "    println(s\"${property.name} recieved a pass from ${property.inDeg} people\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pregraph = org.apache.spark.graphx.impl.GraphImpl@50cdd15b\n",
       "passAndPrGraph = org.apache.spark.graphx.impl.GraphImpl@45ad09e1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@45ad09e1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pregraph = PlayerGraph.staticPageRank(11).cache\n",
    "val passAndPrGraph = PlayerGraph.outerJoinVertices(pregraph.vertices) { (v, pass, rank) => (rank.getOrElse(0.0), pass) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: ClassNotFound with classloader: scala.reflect.internal.util.ScalaClassLoader$URLClassLoader@6c00de9f\n",
       "StackTrace:   at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1439)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1426)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$top$1.apply(RDD.scala:1404)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.top(RDD.scala:1403)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passAndPrGraph.vertices.top(11) {\n",
    "    Ordering.by((entry: (VertexId, (Double, Player))) => entry._2._1)\n",
    "}.foreach( t => println(t._2._2 + \": \" + t._2._1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
