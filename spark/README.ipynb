{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使い方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ブート\n",
    "\n",
    "```\n",
    "# docker-compose build\n",
    "# docker-compose up -d --scale worker=3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark-shellをmasterで動かしてみる\n",
    "\n",
    "```\n",
    "# docker-compose exec master spark-shell --master spark://master:7077\n",
    "  :\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Spark context Web UI available at http://d8a58cf442fd:4040\n",
    "Spark context available as 'sc' (master = spark://master:7077, app id = app-20190717075809-0007).\n",
    "Spark session available as 'spark'.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_212)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala> sc.master\n",
    "res0: String = spark://master:7077\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pysparkを動かしてみる\n",
    "\n",
    "```\n",
    "# docker-compose exec jupyter python\n",
    "Python 3.7.4 (default, Jul 12 2019, 01:16:06)\n",
    "[GCC 8.3.0] on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> from pyspark.context import SparkContext\n",
    ">>> sc = SparkContext(\"spark://master:7077\")\n",
    "19/07/17 08:23:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    ">>> sc.master\n",
    "'spark://master:7077'\n",
    ">>> txt=sc.textFile(\"/spark/README.md\")\n",
    ">>> txt\n",
    "/spark/README.md MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0\n",
    ">>> res = txt.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a,b: a+b)\n",
    ">>> out = dict(res.collect())\n",
    ">>> out\n",
    "{'': 72, './bin/run-example': 2, 'SparkPi': 2, 'run': 7, 'set': 2, 'variable': 1, 'when': 1, 'examples': 2, 'spark://': 1, 'URL,':...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook\n",
    "\n",
    "- open http://localhost:8888\n",
    "- examples\n",
    "  - [sum1](./sum1.ipynb)\n",
    "  - [wordcount-scala](./wordcount-scala.ipynb)\n",
    "  - [runjob](./runjob.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
