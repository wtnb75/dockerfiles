{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "akey = minio\n",
       "skey = miniominio\n",
       "endpoint = http://storage:9000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "http://storage:9000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val akey = \"minio\"\n",
    "val skey = \"miniominio\"\n",
    "val endpoint = \"http://storage:9000\"\n",
    "sc.hadoopConfiguration.set(\"fs.s3n.awsAccessKeyId\", akey)\n",
    "sc.hadoopConfiguration.set(\"fs.s3n.awsSecretAccessKey\", skey)\n",
    "sc.hadoopConfiguration.set(\"fs.s3n.endpoint\", endpoint)\n",
    "sc.hadoopConfiguration.set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "sc.hadoopConfiguration.set(\"fs.s3.awsAccessKeyId\", akey)\n",
    "sc.hadoopConfiguration.set(\"fs.s3.awsSecretAccessKey\", skey)\n",
    "sc.hadoopConfiguration.set(\"fs.s3.endpoint\", endpoint)\n",
    "sc.hadoopConfiguration.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3.S3FileSystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.hadoopConfiguration.set(\"fs.s3a.awsAccessKeyId\", akey)\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.awsSecretAccessKey\", skey)\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.endpoint\", endpoint)\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.access.key\", akey)\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.secret.key\", skey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "txt = s3a://bkt1/README.md MapPartitionsRDD[8] at textFile at <console>:28\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "s3a://bkt1/README.md MapPartitionsRDD[8] at textFile at <console>:28"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val txt = sc.textFile(\"s3a://bkt1/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.RuntimeException\n",
       "Message: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
       "StackTrace:   at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n",
       "  at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n",
       "  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n",
       "  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n",
       "  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n",
       "  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n",
       "  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n",
       "  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n",
       "  at org.apache.spark.internal.io.SparkHadoopWriterUtils$.createPathFromString(SparkHadoopWriterUtils.scala:55)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1066)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n",
       "  ... 42 elided\n",
       "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
       "  at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)\n",
       "  at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.saveAsTextFile(\"s3a://bkt1/README2.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.RuntimeException\n",
       "Message: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
       "StackTrace:   at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n",
       "  at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n",
       "  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n",
       "  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n",
       "  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n",
       "  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n",
       "  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n",
       "  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n",
       "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n",
       "  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n",
       "  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n",
       "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:927)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:925)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.foreach(RDD.scala:925)\n",
       "  ... 42 elided\n",
       "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
       "  at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2101)\n",
       "  at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.foreach(line => {println(\"line: \" + line)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
